\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l | ccc | c cc | cc }
        \toprule
        \multirow{2}{*}[-2mm]{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Raw metrics}} & \multicolumn{5}{c}{\textbf{Aggregated metrics}} \\
             & S & NS & I & Prec & $R_{64}$ & $R_{178}$ & \metricat{64} & \metricat{178} \\
        \midrule
            \geminiultra{} & 83.4 & 13.1 & 7.6 & 86.2 & 98.3 & 46.9 & 91.7 & 60.3 \\
            \geminipro{} & 66.6 & 12.1 & 5.5 & 82.0 & 88.5 & 37.4 & 83.7 & 50.4 \\
        \midrule
            \gptfourturbo{} & 93.6 & 8.3 & 6.1 & 91.7 & 99.0 & 52.6 & 95.0 & 66.4 \\
            \gptfour{} & 59.1 & 7.0 & 2.4 & 89.4 & 88.3 & 33.2 & 88.0 & 48.0 \\
            \chatgpt{} & 46.9 & 4.5 & 1.2 & 90.8 & 72.4 & 26.4 & 79.6 & 40.5 \\
        \midrule
            \claudeopus{} & 63.8 & 8.1 & 2.8 & 88.5 & 91.4 & 35.9 & 89.3 & 50.6 \\
            \claudesonnet{} & 65.4 & 8.4 & 2.6 & 88.5 & 91.4 & 36.7 & 89.4 & 51.4 \\
            \claudehaiku{} & 39.8 & 2.8 & 1.3 & 92.8 & 62.0 & 22.4 & 73.5 & 35.8 \\
            \claudetwopointone{} & 38.3 & 6.5 & 1.9 & 84.8 & 59.8 & 21.5 & 67.9 & 33.7 \\
            \claudetwo{} & 38.5 & 6.3 & 1.3 & 85.5 & 60.0 & 21.6 & 68.7 & 34.0 \\
            \claudeinstant{} & 43.9 & 8.1 & 1.3 & 84.4 & 68.4 & 24.6 & 73.8 & 37.6 \\
        \midrule
            \ulmlitrlhf{} & 72.9 & 8.7 & 4.3 & 89.1 & 94.0 & 41.0 & 91.0 & 55.3 \\
            \ulmlit{} & 13.2 & 1.8 & 0.1 & 88.8 & 20.6 & 7.4 & 31.1 & 13.2 \\
        \bottomrule
    \end{tabular}
    \caption{
        Full benchmarking results on the same 250 random prompts from \longfactobjects{} for the thirteen language models evaluated in \cref{sec:main-results}.
        Metrics are averaged over all prompts.
        Raw metrics are obtained from evaluating responses using \safe{} (see \cref{sec:safe-main}).
        S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively.
        Aggregated metrics are precision (Prec), recall ($R_K$), and \metric{} at $K=64$ and $K=178$ (see \cref{sec:main-metric}).
    }
    \label{tab:model-benchmark-numbers}
\end{table}