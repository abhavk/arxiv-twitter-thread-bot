@misc{zhang2023siren,
  title={Siren's Song in the {AI} Ocean: A Survey on Hallucination in Large Language Models},
  author={Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  year={2023},
  url={https://arxiv.org/abs/2309.01219},
}

@misc{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year={2023},
  url={https://arxiv.org/abs/2307.09288},
}

@misc{lightman2023let,
  title={Let's Verify Step by Step}, 
  author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  year={2023},
  url={https://arxiv.org/abs/2305.20050},
}

@misc{elaraby2023halo,
  title={Halo: Estimation and reduction of hallucinations in open-source weak large language models},
  author={Mohamed Elaraby and Mengyin Lu and Jacob Dunn and Xueying Zhang and Yu Wang and Shizhu Liu and Pingchuan Tian and Yuping Wang and Yuxuan Wang},
  year={2023},
  url={https://arxiv.org/abs/2308.11764},
}

@misc{dhuliawala2023chain,
  title={Chain-of-verification reduces hallucination in large language models},
  author={Shehzaad Dhuliawala and Mojtaba Komeili and Jing Xu and Roberta Raileanu and Xian Li and Asli Celikyilmaz and Jason Weston},
  year={2023},
  url={https://arxiv.org/abs/2309.11495},
}

@misc{li2023inference,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author={Kenneth Li and Oam Patel and Fernanda Vi√©gas and Hanspeter Pfister and Martin Wattenberg},
  year={2023},
  url={https://arxiv.org/abs/2306.03341},
}

@misc{chuang2023dola,
  title={{DoLa}: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James Glass and Pengcheng He},
  year={2023},
  url={https://arxiv.org/abs/2309.03883},
}

@misc{yao2023react,
  title={{ReAct}: Synergizing Reasoning and Acting in Language Models}, 
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2023},
  url={https://arxiv.org/abs/2210.03629},
}

@misc{peng2023check,
  title={Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback}, 
  author={Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
  year={2023},
  url={https://arxiv.org/abs/2302.12813},
}

@misc{huang2023survey,
  title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
  author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
  year={2023},
  url={https://arxiv.org/abs/2311.05232},
}

@inproceedings{lin2022truthfulqa,
  title={{TruthfulQA}: Measuring How Models Mimic Human Falsehoods}, 
  author={Stephanie Lin and Jacob Hilton and Owain Evans},
  year={2022},
  booktitle={Conference of the Association for Computational Linguistics},
  url={https://arxiv.org/abs/2109.07958}
}

@inproceedings{li2023halueval,
  title={{HaluEval}: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}, 
  author={Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2305.11747}
}

@misc{cheng2023evaluating,
  title={Evaluating Hallucinations in Chinese Large Language Models}, 
  author={Qinyuan Cheng and Tianxiang Sun and Wenwei Zhang and Siyin Wang and Xiangyang Liu and Mozhi Zhang and Junliang He and Mianqiu Huang and Zhangyue Yin and Kai Chen and Xipeng Qiu},
  year={2023},
  url={https://arxiv.org/abs/2310.03368},
}

@inproceedings{chen2023felm,
  title={{FELM}: Benchmarking Factuality Evaluation of Large Language Models}, 
  author={Shiqi Chen and Yiran Zhao and Jinghan Zhang and I-Chun Chern and Siyang Gao and Pengfei Liu and Junxian He},
  year={2023},
  booktitle={Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/2310.00741},
}

@misc{vu2023freshllms,
  title={{FreshLLMs}: Refreshing Large Language Models with Search Engine Augmentation}, 
  author={Tu Vu and Mohit Iyyer and Xuezhi Wang and Noah Constant and Jerry Wei and Jason Wei and Chris Tar and Yun-Hsuan Sung and Denny Zhou and Quoc Le and Thang Luong},
  year={2023},
  url={https://arxiv.org/abs/2310.03214},
}

@inproceedings{min2023factscore,
  title={{FActScore}: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, 
  author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2305.14251},
}

@misc{openai2023gpt4,
  title={{GPT}-4 Technical Report}, 
  author={OpenAI},
  year={2023},
  url={https://arxiv.org/abs/2303.08774}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Conference on Neural Information Processing Systems},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@misc{chowdhery2022palm,
    title={Pa{LM}: Scaling Language Modeling with {P}athways},
    author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten  Bosma and Gaurav Mishra and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and others},
    year={2022},
    url={https://arxiv.org/abs/2204.02311}
}

@misc{anil2023palm,
  title={{PaLM} 2 Technical Report}, 
  author={Google},
  year={2023},
  url={https://arxiv.org/abs/2305.10403},
}

@misc{gemini2023,
    title={Gemini: A family of highly capable multimodal models},
    author={Gemini Team, Google},
    year={2023},
    url={https://arxiv.org/abs/2312.11805},
}

@inproceedings{muhlgay2023generating,
  title={Generating Benchmarks for Factuality Evaluation of Language Models}, 
  author={Dor Muhlgay and Ori Ram and Inbal Magar and Yoav Levine and Nir Ratner and Yonatan Belinkov and Omri Abend and Kevin Leyton-Brown and Amnon Shashua and Yoav Shoham},
  year={2023},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  url={https://arxiv.org/abs/2307.06908},
}

@inproceedings{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2203.11171},
}

@misc{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  year={2022},
  url={https://arxiv.org/abs/2210.11416},
}

@misc{chen2023universal,
  title={Universal Self-Consistency for Large Language Model Generation}, 
  author={Xinyun Chen and Renat Aksitov and Uri Alon and Jie Ren and Kefan Xiao and Pengcheng Yin and Sushant Prakash and Charles Sutton and Xuezhi Wang and Denny Zhou},
  year={2023},
  url={https://arxiv.org/abs/2311.17311},
}

@misc{tang2023middle,
  title={Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models}, 
  author={Raphael Tang and Xinyu Zhang and Xueguang Ma and Jimmy Lin and Ferhan Ture},
  year={2023},
  url={https://arxiv.org/abs/2310.07712},
}

@inproceedings{mitchell2022enhancing,
  title={Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference}, 
  author={Eric Mitchell and Joseph J. Noh and Siyan Li and William S. Armstrong and Ananth Agarwal and Patrick Liu and Chelsea Finn and Christopher D. Manning},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2211.11875},
}

@misc{huang2023enhancing,
  title={Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency}, 
  author={Baizhou Huang and Shuai Lu and Weizhu Chen and Xiaojun Wan and Nan Duan},
  year={2023},
  url={https://arxiv.org/abs/2309.17272}
}

@misc{wang2023knowledgetuning,
  title={Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese}, 
  author={Haochun Wang and Sendong Zhao and Zewen Qiang and Zijian Li and Nuwa Xi and Yanrui Du and MuZhen Cai and Haoqiang Guo and Yuhan Chen and Haoming Xu and Bing Qin and Ting Liu},
  year={2023},
  url={https://arxiv.org/abs/2309.04175}
}

@misc{li2023structured,
  title={Structured Chain-of-Thought Prompting for Code Generation}, 
  author={Jia Li and Ge Li and Yongmin Li and Zhi Jin},
  year={2023},
  url={https://arxiv.org/abs/2305.06599},
}

@inproceedings{wei2022chainofthought,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year={2023},
  booktitle={Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/2201.11903},
}

@inproceedings{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding}, 
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  year={2021},
  booktitle={International Conference on Learning Representations},
  url={https://arxiv.org/abs/2009.03300},
}

@inproceedings{li2022multispanqa,
  title={{MultiSpanQA}: A Dataset for Multi-Span Question Answering},
  author={Haonan Li and Maria Vasardani and Martin Tomko and Timothy Baldwin},
  year={2022},
  booktitle={Conference of the North American Chapter of the Association for Computational Linguistics},
  url={https://aclanthology.org/2022.naacl-main.90.pdf}
}

@misc{es2023ragas,
  title={{RAGAS}: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  url={https://arxiv.org/abs/2309.15217},
  year={2023}
}

@misc{guan2023language,
  title={Language Models Hallucinate, but May Excel at Fact Verification},
  author={Guan, Jian and Dodge, Jesse and Wadden, David and Huang, Minlie and Peng, Hao},
  url={https://arxiv.org/abs/2310.14564},
  year={2023}
}

@misc{cheng2023relic,
  title={{RELIC}: Investigating Large Language Model Responses using Self-Consistency},
  author={Cheng, Furui and Zouhar, Vil{\'e}m and Arora, Simran and Sachan, Mrinmaya and Strobelt, Hendrik and El-Assady, Mennatallah},
  url={https://arxiv.org/abs/2311.16842},
  year={2023}
}

@inproceedings{sellam2020bleurt,
  title={{BLEURT}: Learning Robust Metrics for Text Generation}, 
  author={Thibault Sellam and Dipanjan Das and Ankur P. Parikh},
  year={2020},
  booktitle={Conference of the Association for Computational Linguistics},
  url={https://arxiv.org/abs/2004.04696},
}

@inproceedings{lin2004rouge,
    title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
    author = {Lin, Chin-Yew},
    year = {2004},
    booktitle={Conference of the Association for Computational Linguistics},
    url = {https://aclanthology.org/W04-1013},
}

@misc{ganesan2018rouge,
  author       = {Kavita Ganesan},
  title        = {{ROUGE} 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.01937},
}

@inproceedings{wadden2022scifactopen,
  title={{SciFact-Open}: Towards open-domain scientific claim verification}, 
  author={David Wadden and Kyle Lo and Bailey Kuehl and Arman Cohan and Iz Beltagy and Lucy Lu Wang and Hannaneh Hajishirzi},
  year={2022},
  booktitle = {Findings of the Association for Computational Linguistics: Conference on Empirical Methods in Natural Language Processing 2022},
  url = {https://arxiv.org/abs/2210.13777}
}

@misc{chern2023factool,
  title={{FacTool}: Factuality Detection in Generative {AI} -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios}, 
  author={I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},
  year={2023},
  url={https://arxiv.org/abs/2307.13528},
}

@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2016},
  url={https://arxiv.org/abs/1606.05250},
}

@inproceedings{wieting2021paraphrastic,
  title={Paraphrastic representations at scale},
  author={Wieting, John and Gimpel, Kevin and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2104.15114},
  year={2022}
}

@misc{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  url={https://arxiv.org/abs/2311.08401},
  year={2023}
}

@inproceedings{manakul2023selfcheckgpt,
  title={{SelfCheckGPT}: Zero-resource black-box hallucination detection for generative large language models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark JF},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2303.08896},
  year={2023}
}

@misc{chatgpt,
  title={Introducing {ChatGPT}},
  author={OpenAI},
  year={2022},
  url={https://openai.com/blog/chatgpt},
}

@misc{claude3,
  title={Introducing the next generation of {Claude}},
  author={Anthropic},
  year={2024},
  url={https://www.anthropic.com/news/claude-3-family}
}

@misc{claude2,
  title={Claude 2},
  author={Anthropic},
  year={2023},
  url={https://www.anthropic.com/news/claude-2},
}

@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences}, 
  author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
  year={2017},
  booktitle={Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/1706.03741},
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback}, 
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  year={2022},
  booktitle={Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/2203.02155}
}

@misc{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} Feedback}, 
  author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
  year={2022},
  url={https://arxiv.org/abs/2212.08073},
}

@inproceedings{zheng2023judging,
  title={Judging {LLM}-as-a-{J}udge with {MT}-{B}ench and Chatbot Arena}, 
  author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
  year={2023},
  booktitle={Conference on Neural Information Processing Systems},
  url={https://arxiv.org/abs/2306.05685},
}

@misc{wei2023larger,
  title={Larger language models do in-context learning differently}, 
  author={Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  year={2023},
  url={https://arxiv.org/abs/2303.03846},
}

@inproceedings{wei2023symbol,
  title={Symbol tuning improves in-context learning in language models}, 
  author={Jerry Wei and Le Hou and Andrew Lampinen and Xiangning Chen and Da Huang and Yi Tay and Xinyun Chen and Yifeng Lu and Denny Zhou and Tengyu Ma and Quoc V. Le},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://arxiv.org/abs/2305.08298},
}

@misc{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with {GPT}-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  year={2023},
  url={https://arxiv.org/abs/2303.12712},
}

@article{li2022competition,
  title={Competition-level code generation with {AlphaCode}},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  year={2022},
  url={https://arxiv.org/abs/2203.07814},
}

@misc{pu2023summarization,
  title={Summarization is (almost) dead},
  author={Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  year={2023},
  url={https://arxiv.org/abs/2309.09558},
}

@misc{koo2023benchmarking,
  title={Benchmarking Cognitive Biases in Large Language Models as Evaluators}, 
  author={Ryan Koo and Minhwa Lee and Vipul Raheja and Jong Inn Park and Zae Myung Kim and Dongyeop Kang},
  year={2023},
  url={https://arxiv.org/abs/2309.17012},
}

@misc{xu2024perils,
  title={Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models}, 
  author={Wenda Xu and Guanglei Zhu and Xuandong Zhao and Liangming Pan and Lei Li and William Yang Wang},
  year={2024},
  url={https://arxiv.org/abs/2402.11436},
}

@misc{augenstein2023factuality,
  title={Factuality Challenges in the Era of Large Language Models}, 
  author={Isabelle Augenstein and Timothy Baldwin and Meeyoung Cha and Tanmoy Chakraborty and Giovanni Luca Ciampaglia and David Corney and Renee DiResta and Emilio Ferrara and Scott Hale and Alon Halevy and Eduard Hovy and Heng Ji and Filippo Menczer and Ruben Miguez and Preslav Nakov and Dietram Scheufele and Shivam Sharma and Giovanni Zagni},
  year={2023},
  url={https://arxiv.org/abs/2310.05189}
}

@inproceedings{gao2023rarr,
  title={RARR: Researching and Revising What Language Models Say, Using Language Models}, 
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2023},
  url={https://arxiv.org/abs/2210.08726},
  booktitle={Conference of the Association for Computational Linguistics},
}