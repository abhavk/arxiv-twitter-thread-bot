\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthropic(2023)]{claude2}
Anthropic.
\newblock Claude 2, 2023.
\newblock URL \url{https://www.anthropic.com/news/claude-2}.

\bibitem[Anthropic(2024)]{claude3}
Anthropic.
\newblock Introducing the next generation of {Claude}, 2024.
\newblock URL \url{https://www.anthropic.com/news/claude-3-family}.

\bibitem[Augenstein et~al.(2023)Augenstein, Baldwin, Cha, Chakraborty,
  Ciampaglia, Corney, DiResta, Ferrara, Hale, Halevy, Hovy, Ji, Menczer,
  Miguez, Nakov, Scheufele, Sharma, and Zagni]{augenstein2023factuality}
Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty,
  Giovanni~Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott
  Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez,
  Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni.
\newblock Factuality challenges in the era of large language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.05189}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli,
  Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite,
  Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson,
  Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly,
  Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish,
  Brown, and Kaplan]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,
  Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
  Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,
  Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi
  Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott
  Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham,
  Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R.
  Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam
  McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional {AI}: Harmlessness from {AI} feedback, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.08073}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  {GPT}-4, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.12712}.

\bibitem[Chen et~al.(2023)Chen, Zhao, Zhang, Chern, Gao, Liu, and
  He]{chen2023felm}
Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu,
  and Junxian He.
\newblock {FELM}: Benchmarking factuality evaluation of large language models.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.00741}.

\bibitem[Cheng et~al.(2023{\natexlab{a}})Cheng, Zouhar, Arora, Sachan,
  Strobelt, and El-Assady]{cheng2023relic}
Furui Cheng, Vil{\'e}m Zouhar, Simran Arora, Mrinmaya Sachan, Hendrik Strobelt,
  and Mennatallah El-Assady.
\newblock {RELIC}: Investigating large language model responses using
  self-consistency, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2311.16842}.

\bibitem[Cheng et~al.(2023{\natexlab{b}})Cheng, Sun, Zhang, Wang, Liu, Zhang,
  He, Huang, Yin, Chen, and Qiu]{cheng2023evaluating}
Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi
  Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu.
\newblock Evaluating hallucinations in chinese large language models,
  2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2310.03368}.

\bibitem[Chern et~al.(2023)Chern, Chern, Chen, Yuan, Feng, Zhou, He, Neubig,
  and Liu]{chern2023factool}
I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou,
  Junxian He, Graham Neubig, and Pengfei Liu.
\newblock {FacTool}: Factuality detection in generative {AI} -- a tool
  augmented framework for multi-task and multi-domain scenarios, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.13528}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Chung,
  Sutton, Gehrmann, Schuh, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et~al.
\newblock Pa{LM}: Scaling language modeling with {P}athways, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03741}.

\bibitem[Dhuliawala et~al.(2023)Dhuliawala, Komeili, Xu, Raileanu, Li,
  Celikyilmaz, and Weston]{dhuliawala2023chain}
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli
  Celikyilmaz, and Jason Weston.
\newblock Chain-of-verification reduces hallucination in large language models,
  2023.
\newblock URL \url{https://arxiv.org/abs/2309.11495}.

\bibitem[Es et~al.(2023)Es, James, Espinosa-Anke, and Schockaert]{es2023ragas}
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert.
\newblock {RAGAS}: Automated evaluation of retrieval augmented generation,
  2023.
\newblock URL \url{https://arxiv.org/abs/2309.15217}.

\bibitem[Ganesan(2018)]{ganesan2018rouge}
Kavita Ganesan.
\newblock {ROUGE} 2.0: Updated and improved measures for evaluation of
  summarization tasks, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.01937}.

\bibitem[Gao et~al.(2023)Gao, Dai, Pasupat, Chen, Chaganty, Fan, Zhao, Lao,
  Lee, Juan, and Guu]{gao2023rarr}
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun~Tejasvi Chaganty,
  Yicheng Fan, Vincent~Y. Zhao, Ni~Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin
  Guu.
\newblock Rarr: Researching and revising what language models say, using
  language models.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2023.
\newblock URL \url{https://arxiv.org/abs/2210.08726}.

\bibitem[Gemini~Team(2023)]{gemini2023}
Google Gemini~Team.
\newblock Gemini: A family of highly capable multimodal models, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.11805}.

\bibitem[Google(2023)]{anil2023palm}
Google.
\newblock {PaLM} 2 technical report, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.10403}.

\bibitem[Guan et~al.(2023)Guan, Dodge, Wadden, Huang, and
  Peng]{guan2023language}
Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, and Hao Peng.
\newblock Language models hallucinate, but may excel at fact verification,
  2023.
\newblock URL \url{https://arxiv.org/abs/2310.14564}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2021mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://arxiv.org/abs/2009.03300}.

\bibitem[Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng,
  Qin, and Liu]{huang2023survey}
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang,
  Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu.
\newblock A survey on hallucination in large language models: Principles,
  taxonomy, challenges, and open questions, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.05232}.

\bibitem[Koo et~al.(2023)Koo, Lee, Raheja, Park, Kim, and
  Kang]{koo2023benchmarking}
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong~Inn Park, Zae~Myung Kim, and Dongyeop
  Kang.
\newblock Benchmarking cognitive biases in large language models as evaluators,
  2023.
\newblock URL \url{https://arxiv.org/abs/2309.17012}.

\bibitem[Li et~al.(2023)Li, Cheng, Zhao, Nie, and Wen]{li2023halueval}
Junyi Li, Xiaoxue Cheng, Wayne~Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
\newblock {HaluEval}: A large-scale hallucination evaluation benchmark for
  large language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.11747}.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond,
  Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  et~al.
\newblock Competition-level code generation with {AlphaCode}.
\newblock \emph{Science}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.07814}.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2004.
\newblock URL \url{https://aclanthology.org/W04-1013}.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock {TruthfulQA}: Measuring how models mimic human falsehoods.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2022.
\newblock URL \url{https://arxiv.org/abs/2109.07958}.

\bibitem[Manakul et~al.(2023)Manakul, Liusie, and
  Gales]{manakul2023selfcheckgpt}
Potsawee Manakul, Adian Liusie, and Mark~JF Gales.
\newblock {SelfCheckGPT}: Zero-resource black-box hallucination detection for
  generative large language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.08896}.

\bibitem[Min et~al.(2023)Min, Krishna, Lyu, Lewis, tau Yih, Koh, Iyyer,
  Zettlemoyer, and Hajishirzi]{min2023factscore}
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang~Wei Koh,
  Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {FActScore}: Fine-grained atomic evaluation of factual precision in
  long form text generation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.14251}.

\bibitem[Muhlgay et~al.(2023)Muhlgay, Ram, Magar, Levine, Ratner, Belinkov,
  Abend, Leyton-Brown, Shashua, and Shoham]{muhlgay2023generating}
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov,
  Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham.
\newblock Generating benchmarks for factuality evaluation of language models.
\newblock In \emph{Conference of the European Chapter of the Association for
  Computational Linguistics}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.06908}.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Introducing {ChatGPT}, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Pu et~al.(2023)Pu, Gao, and Wan]{pu2023summarization}
Xiao Pu, Mingqi Gao, and Xiaojun Wan.
\newblock Summarization is (almost) dead, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.09558}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.05250}.

\bibitem[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt}
Thibault Sellam, Dipanjan Das, and Ankur~P. Parikh.
\newblock {BLEURT}: Learning robust metrics for text generation.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.04696}.

\bibitem[Tian et~al.(2023)Tian, Mitchell, Yao, Manning, and Finn]{tian2023fine}
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher~D Manning, and Chelsea
  Finn.
\newblock Fine-tuning language models for factuality, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.08401}.

\bibitem[Vu et~al.(2023)Vu, Iyyer, Wang, Constant, Wei, Wei, Tar, Sung, Zhou,
  Le, and Luong]{vu2023freshllms}
Tu~Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris
  Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong.
\newblock {FreshLLMs}: Refreshing large language models with search engine
  augmentation, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.03214}.

\bibitem[Wadden et~al.(2022)Wadden, Lo, Kuehl, Cohan, Beltagy, Wang, and
  Hajishirzi]{wadden2022scifactopen}
David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz~Beltagy, Lucy~Lu Wang, and
  Hannaneh Hajishirzi.
\newblock {SciFact-Open}: Towards open-domain scientific claim verification.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  Conference on Empirical Methods in Natural Language Processing 2022}, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.13777}.

\bibitem[Wei et~al.(2023{\natexlab{a}})Wei, Wang, Schuurmans, Bosma, Ichter,
  Xia, Chi, Le, and Zhou]{wei2022chainofthought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Conference on Neural Information Processing Systems},
  2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2201.11903}.

\bibitem[Wei et~al.(2023{\natexlab{b}})Wei, Hou, Lampinen, Chen, Huang, Tay,
  Chen, Lu, Zhou, Ma, and Le]{wei2023symbol}
Jerry Wei, Le~Hou, Andrew Lampinen, Xiangning Chen, Da~Huang, Yi~Tay, Xinyun
  Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc~V. Le.
\newblock Symbol tuning improves in-context learning in language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2305.08298}.

\bibitem[Wei et~al.(2023{\natexlab{c}})Wei, Wei, Tay, Tran, Webson, Lu, Chen,
  Liu, Huang, Zhou, and Ma]{wei2023larger}
Jerry Wei, Jason Wei, Yi~Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun
  Chen, Hanxiao Liu, Da~Huang, Denny Zhou, and Tengyu Ma.
\newblock Larger language models do in-context learning differently,
  2023{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2303.03846}.

\bibitem[Wieting et~al.(2022)Wieting, Gimpel, Neubig, and
  Berg-Kirkpatrick]{wieting2021paraphrastic}
John Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-Kirkpatrick.
\newblock Paraphrastic representations at scale.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2022.
\newblock URL \url{https://arxiv.org/abs/2104.15114}.

\bibitem[Xu et~al.(2024)Xu, Zhu, Zhao, Pan, Li, and Wang]{xu2024perils}
Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William~Yang
  Wang.
\newblock Perils of self-feedback: Self-bias amplifies in large language
  models, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.11436}.

\bibitem[Zhang et~al.(2023)Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang,
  Chen, Wang, Luu, Bi, Shi, and Shi]{zhang2023siren}
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
  Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, Longyue Wang, Anh~Tuan Luu, Wei Bi,
  Freda Shi, and Shuming Shi.
\newblock Siren's song in the {AI} ocean: A survey on hallucination in large
  language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.01219}.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric~P. Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging {LLM}-as-a-{J}udge with {MT}-{B}ench and chatbot arena.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.05685}.

\end{thebibliography}
