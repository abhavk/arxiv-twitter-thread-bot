\begin{thebibliography}{58}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{et~al.(2022)}]{bigbench}
Aarohi~Srivastava et~al. 2022.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{ArXiv}, abs/2206.04615.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock In \emph{Proceedings of the Conference on Advances in Neural
  Information Processing Systems (NeurIPS)}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry,
  Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet,
  Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol,
  Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike,
  Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder,
  McGrew, Amodei, McCandlish, Sutskever, and Zaremba}]{codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Pond{\'{e}}
  de~Oliveira~Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
  Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
  Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
  Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
  Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings,
  Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert{-}Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba. 2021.
\newblock \href {http://arxiv.org/abs/2107.03374} {Evaluating large language
  models trained on code}.
\newblock \emph{CoRR}, abs/2107.03374.

\bibitem[{Chen et~al.(2022)Chen, Ma, Wang, and Cohen}]{progcot}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen. 2022.
\newblock Program of thoughts prompting: Disentangling computation from
  reasoning for numerical reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.12588}.

\bibitem[{Chen et~al.(2023)Chen, Lin, Sch{\"a}rli, and Zhou}]{chen2023teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou. 2023.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph,
  Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat,
  Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat,
  Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek~Baindoor Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar
  Prabhakaran, Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James
  Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
  Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
  Garc{\'i}a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne
  Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan
  Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
  Erica~Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,
  Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele
  Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
  Petrov, and Noah Fiedel. 2022.
\newblock {PaLM: Scaling Language Modeling with Pathways}.
\newblock \emph{ArXiv}, abs/2204.02311.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Creswell et~al.(2023)Creswell, Shanahan, and
  Higgins}]{creswell2023selectioninference}
Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023.
\newblock \href {https://openreview.net/forum?id=3Pf3Wg6o-A4}
  {Selection-inference: Exploiting large language models for interpretable
  logical reasoning}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Davis and Putnam(1960)}]{resolution}
Martin Davis and Hilary Putnam. 1960.
\newblock \href {https://doi.org/10.1145/321033.321034} {A computing procedure
  for quantification theory}.
\newblock \emph{J. ACM}, 7(3):201–215.

\bibitem[{De~Moura and Bj\o{}rner(2008)}]{z3}
Leonardo De~Moura and Nikolaj Bj\o{}rner. 2008.
\newblock {Z3: An Efficient SMT Solver}.
\newblock In \emph{Proceedings of the Theory and Practice of Software, 14th
  International Conference on Tools and Algorithms for the Construction and
  Analysis of Systems}, TACAS'08/ETAPS'08, page 337–340, Berlin, Heidelberg.
  Springer-Verlag.

\bibitem[{Demeter and Downey(2020)}]{neuralsymlm}
David Demeter and Doug Downey. 2020.
\newblock \href {https://doi.org/10.1609/aaai.v34i05.6264} {Just add functions:
  A neural-symbolic language model}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34(05):7634--7642.

\bibitem[{El-Yaniv and Wiener(2010)}]{selectivesetting}
Ran El-Yaniv and Yair Wiener. 2010.
\newblock \href {http://jmlr.org/papers/v11/el-yaniv10a.html} {On the
  foundations of noise-free selective classification}.
\newblock \emph{Journal of Machine Learning Research}, 11(53):1605--1641.

\bibitem[{Fu et~al.(2022)Fu, Peng, Sabharwal, Clark, and
  Khot}]{fu2022complexity}
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022.
\newblock Complexity-based prompting for multi-step reasoning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and
  Neubig}]{pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
  Callan, and Graham Neubig. 2023.
\newblock Pal: Program-aided language models.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}.

\bibitem[{Garcez and Lamb(2023)}]{garcez2023neurosymbolic}
Artur~d’Avila Garcez and Luis~C Lamb. 2023.
\newblock {Neurosymbolic AI: The 3rd wave}.
\newblock \emph{Artificial Intelligence Review}, pages 1--20.

\bibitem[{Gehrmann et~al.(2021)Gehrmann, Adewumi, Aggarwal, Ammanamanchi,
  Aremu, Bosselut, Chandu, Clinciu, Das, Dhole, Du, Durmus, Du{\v{s}}ek,
  Emezue, Gangal, Garbacea, Hashimoto, Hou, Jernite, Jhamtani, Ji, Jolly, Kale,
  Kumar, Ladhak, Madaan, Maddela, Mahajan, Mahamood, Majumder, Martins,
  McMillan-Major, Mille, van Miltenburg, Nadeem, Narayan, Nikolaev,
  Niyongabo~Rubungo, Osei, Parikh, Perez-Beltrachini, Rao, Raunak, Rodriguez,
  Santhanam, Sedoc, Sellam, Shaikh, Shimorina, Sobrevilla~Cabezudo, Strobelt,
  Subramani, Xu, Yang, Yerukola, and Zhou}]{gem}
Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan~Sasanka
  Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi~Raghavi Chandu,
  Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus,
  Ond{\v{r}}ej Du{\v{s}}ek, Chris~Chinenye Emezue, Varun Gangal, Cristina
  Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani,
  Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman
  Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa~Prasad
  Majumder, Pedro~Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel
  van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre
  Niyongabo~Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini,
  Niranjan~Ramesh Rao, Vikas Raunak, Juan~Diego Rodriguez, Sashank Santhanam,
  Jo{\~a}o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina,
  Marco~Antonio Sobrevilla~Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei
  Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.gem-1.10} {The {GEM}
  benchmark: Natural language generation, its evaluation and metrics}.
\newblock In \emph{Proceedings of the 1st Workshop on Natural Language
  Generation, Evaluation, and Metrics (GEM 2021)}, pages 96--120, Online.
  Association for Computational Linguistics.

\bibitem[{Gonen et~al.(2022)Gonen, Iyer, Blevins, Smith, and
  Zettlemoyer}]{hila2022}
Hila Gonen, Srini Iyer, Terra Blevins, Noah~A Smith, and Luke Zettlemoyer.
  2022.
\newblock Demystifying prompts in language models via perplexity estimation.
\newblock \emph{arXiv preprint arXiv:2212.04037}.

\bibitem[{He-Yueya et~al.(2023)He-Yueya, Poesia, Wang, and Goodman}]{gsmsat}
Joy He-Yueya, Gabriel Poesia, Rose~E. Wang, and Noah~D. Goodman. 2023.
\newblock Solving math word problems by combining language models with symbolic
  solvers.
\newblock \emph{ArXiv}, abs/2304.09102.

\bibitem[{Jain et~al.(2022)Jain, Vaidyanath, Iyer, Natarajan, Parthasarathy,
  Rajamani, and Sharma}]{jigsaw}
Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh
  Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022.
\newblock Jigsaw: Large language models meet program synthesis.
\newblock \emph{ICSE}.

\bibitem[{Kazemi et~al.(2023)Kazemi, Yuan, Bhatia, Kim, Xu, Imbrasaite, and
  Ramachandran}]{boardgameqa}
Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite,
  and Deepak Ramachandran. 2023.
\newblock {BoardgameQA: A Dataset for Natural Language Reasoning with
  Contradictory Information}.
\newblock In \emph{Proceedings of the Conference on Advances in Neural
  Information Processing Systems (NeurIPS)}.

\bibitem[{Khot et~al.(2022)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and
  Sabharwal}]{khot2022decomposed}
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter
  Clark, and Ashish Sabharwal. 2022.
\newblock Decomposed prompting: A modular approach for solving complex tasks.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa}]{zerocot}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa. 2022.
\newblock Large language models are zero-shot reasoners.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Li et~al.(2022{\natexlab{a}})Li, Chen, Shen, Chen, Zhang, Li, Wang,
  Qian, Peng, Mao et~al.}]{li2022explanations}
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong
  Wang, Jing Qian, Baolin Peng, Yi~Mao, et~al. 2022{\natexlab{a}}.
\newblock Explanations from large language models make small reasoners better.
\newblock \emph{arXiv preprint arXiv:2210.06726}.

\bibitem[{Li et~al.(2022{\natexlab{b}})Li, Lin, Zhang, Fu, Chen, Lou, and
  Chen}]{li2022advance}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and
  Weizhu Chen. 2022{\natexlab{b}}.
\newblock On the advance of making language models better reasoners.
\newblock \emph{arXiv preprint arXiv:2206.02336}.

\bibitem[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, Newman, Yuan, Yan, Zhang, Cosgrove, Manning,
  R'e, Acosta-Navas, Hudson, Zelikman, Durmus, Ladhak, Rong, Ren, Yao, Wang,
  Santhanam, Orr, Zheng, Yuksekgonul, Suzgun, Kim, Guha, Chatterji, Khattab,
  Henderson, Huang, Chi, Xie, Santurkar, Ganguli, Hashimoto, Icard, Zhang,
  Chaudhary, Wang, Li, Mai, Zhang, and Koreeda}]{Liang2022HolisticEO}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  Benjamin Newman, Binhang Yuan, Bobby Yan, Ce~Zhang, Christian Cosgrove,
  Christopher~D. Manning, Christopher R'e, Diana Acosta-Navas, Drew~A. Hudson,
  E.~Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao,
  Jue Wang, Keshav Santhanam, Laurel~J. Orr, Lucia Zheng, Mert Yuksekgonul,
  Mirac Suzgun, Nathan~S. Kim, Neel Guha, Niladri~S. Chatterji, Omar Khattab,
  Peter Henderson, Qian Huang, Ryan Chi, Sang~Michael Xie, Shibani Santurkar,
  Surya Ganguli, Tatsunori Hashimoto, Thomas~F. Icard, Tianyi Zhang, Vishrav
  Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta
  Koreeda. 2022.
\newblock Holistic evaluation of language models.
\newblock \emph{ArXiv}, abs/2211.09110.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Jiang, Zhang, Liu, Zhang, Biswas,
  and Stone}]{liu2023llmplanner}
Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas,
  and Peter Stone. 2023{\natexlab{a}}.
\newblock {LLM+ P: Empowering Large Language Models with Optimal Planning
  Proficiency}.
\newblock \emph{arXiv preprint arXiv:2304.11477}.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig}]{promptsurvey}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig. 2023{\natexlab{b}}.
\newblock \href {https://doi.org/10.1145/3560815} {Pre-train, prompt, and
  predict: A systematic survey of prompting methods in natural language
  processing}.
\newblock \emph{ACM Comput. Surv.}, 55(9).

\bibitem[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki,
  and Callison-Burch}]{faithfulcot}
Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna
  Apidianaki, and Chris Callison-Burch. 2023.
\newblock Faithful chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2301.13379}.

\bibitem[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang et~al.}]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
  2023.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}.

\bibitem[{Marcus(2020)}]{marcus2020next}
Gary Marcus. 2020.
\newblock {The next decade in AI: four steps towards robust artificial
  intelligence}.
\newblock \emph{arXiv preprint arXiv:2002.06177}.

\bibitem[{Ni et~al.(2023)Ni, Iyer, Radev, Stoyanov, Yih, Wang, and
  Lin}]{ni2023lever}
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida~I Wang,
  and Xi~Victoria Lin. 2023.
\newblock {LEVER: Learning to Verify Language-to-Code Generation with
  Execution}.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}.

\bibitem[{Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin,
  Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena}]{scratch}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, Charles Sutton, and Augustus Odena. 2021.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{ArXiv}, abs/2112.00114.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{instructgpt}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Proceedings of the Conference on Advances in Neural
  Information Processing Systems (NeurIPS)}.

\bibitem[{Paul et~al.(2023)Paul, Ismayilzada, Peyrard, Borges, Bosselut, West,
  and Faltings}]{paul2023refiner}
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
  Bosselut, Robert West, and Boi Faltings. 2023.
\newblock Refiner: Reasoning feedback on intermediate representations.
\newblock \emph{arXiv preprint arXiv:2304.01904}.

\bibitem[{Poesia et~al.(2022)Poesia, Polozov, Le, Tiwari, Soares, Meek, and
  Gulwani}]{poesia2022synchromesh}
Gabriel Poesia, Alex Polozov, Vu~Le, Ashish Tiwari, Gustavo Soares, Christopher
  Meek, and Sumit Gulwani. 2022.
\newblock \href {https://openreview.net/forum?id=KmtVD97J43e} {Synchromesh:
  Reliable code generation from pre-trained language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving}]{Rae2021ScalingLM}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John F.~J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese,
  Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden,
  Esme Sutherland, Karen Simonyan, Michela Paganini, L.~Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, N.~K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
  de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
  Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones, James
  Bradbury, Matthew~G. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason
  Gabriel, William~S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell,
  Chris Dyer, Oriol Vinyals, Kareem~W. Ayoub, Jeff Stanway, L.~L. Bennett,
  Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.
\newblock {Scaling Language Models: Methods, Analysis \& Insights from Training
  Gopher}.
\newblock \emph{ArXiv}, abs/2112.11446.

\bibitem[{Rahmani et~al.(2021)Rahmani, Raza, Gulwani, Le, Morris, Radhakrishna,
  Soares, and Tiwari}]{marriage}
Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu~Le, Daniel Morris, Arjun
  Radhakrishna, Gustavo Soares, and Ashish Tiwari. 2021.
\newblock \href {https://doi.org/10.1145/3485535} {Multi-modal program
  inference: A marriage of pre-trained language models and component-based
  synthesis}.
\newblock \emph{Proc. ACM Program. Lang.}, 5(OOPSLA).

\bibitem[{Reif et~al.(2022)Reif, Ippolito, Yuan, Coenen, Callison-Burch, and
  Wei}]{recipe}
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and
  Jason Wei. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-short.94} {A recipe for
  arbitrary text style transfer with large language models}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 837--848,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Ribeiro et~al.(2023)Ribeiro, Wang, Ma, Zhu, Dong, Kong, Burger,
  Ramos, zhiheng huang, Wang, Karypis, Xiang, and Roth}]{ribeiro2023street}
Danilo~Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang
  Kong, Juliette Burger, Anjelica Ramos, zhiheng huang, William~Yang Wang,
  George Karypis, Bing Xiang, and Dan Roth. 2023.
\newblock \href {https://openreview.net/forum?id=1C_kSW1-k0} {{STREET}: A
  multi-task structured reasoning and explanation benchmark}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim,
  Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao,
  Biderman, Gao, Wolf, and Rush}]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful
  Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla,
  Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
  Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong,
  Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
  Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan
  Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M
  Rush. 2022.
\newblock \href {https://openreview.net/forum?id=9Vrb9D0WI4} {Multitask
  prompted training enables zero-shot task generalization}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Saparov and He(2023)}]{saparov2023language}
Abulhair Saparov and He~He. 2023.
\newblock \href {https://openreview.net/forum?id=qFVVBzXxR2V} {Language models
  are greedy reasoners: A systematic formal analysis of chain-of-thought}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Schick et~al.(2023)Schick, Dwivedi-Yu, Dessì, Raileanu, Lomeli,
  Zettlemoyer, Cancedda, and Scialom}]{toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
  Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2302.04761} {Toolformer:
  Language models can teach themselves to use tools}.
\newblock \emph{arXiv}.

\bibitem[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and
  Hamilton}]{clutrr}
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L.
  Hamilton. 2019.
\newblock {CLUTRR}: A diagnostic benchmark for inductive reasoning from text.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}.

\bibitem[{Tafjord et~al.(2021)Tafjord, Dalvi, and Clark}]{proofwriter}
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.
\newblock {P}roof{W}riter: Generating implications, proofs, and abductive
  statements over natural language.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP (ACL Findings)}.

\bibitem[{Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and
  Kambhampati}]{Valmeekam2022LargeLM}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
  2022.
\newblock {Large Language Models Still Can't Plan (A Benchmark for LLMs on
  Planning and Reasoning about Change)}.
\newblock \emph{ArXiv}, abs/2206.10498.

\bibitem[{Wang et~al.(2022{\natexlab{a}})Wang, Chan, Ilievski, Chen, and
  Ren}]{pinto}
Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren.
  2022{\natexlab{a}}.
\newblock Pinto: Faithful language reasoning using prompt-generated rationales.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, hsin Chi,
  and Zhou}]{selfcons}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Huai hsin Chi, and Denny
  Zhou. 2022{\natexlab{b}}.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le}]{flan}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le. 2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=gEZrGCozdqR} {Finetuned
  language models are zero-shot learners}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang,
  Dean, and Fedus}]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=yzkSU5zdwD} {Emergent
  abilities of large language models}.
\newblock \emph{Transactions on Machine Learning Research}.
\newblock Survey Certification.

\bibitem[{Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Chi, Le,
  and Zhou}]{chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou. 2022{\natexlab{c}}.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Proceedings of the Conference on Advances in Neural
  Information Processing Systems (NeurIPS)}.

\bibitem[{Ye et~al.(2020)Ye, Chen, Dillig, and Durrett}]{structuredregex}
Xi~Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2020.
\newblock Benchmarking multimodal regex synthesis with complex structures.
\newblock In \emph{Proceedings of the Annual Conference of the Association for
  Computational Linguistics (ACL)}.

\bibitem[{Ye et~al.(2021)Ye, Chen, Dillig, and Durrett}]{opsynth}
Xi~Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2021.
\newblock Optimal neural program synthesis from multimodal specifications.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP (EMNLP Findings)}.

\bibitem[{Ye and Durrett(2023)}]{optexpl}
Xi~Ye and Greg Durrett. 2023.
\newblock Explanation selection using unlabeled data for chain-of-thought
  prompting.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}.

\bibitem[{Yu et~al.(2023)Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and
  Jiang}]{yu2023generate}
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
  Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023.
\newblock Generate rather than retrieve: Large language models are strong
  context generators.
\newblock In \emph{International Conference for Learning Representation
  (ICLR)}.

\bibitem[{Zhang et~al.(2022{\natexlab{a}})Zhang, Li, Huang, Naik, and
  Xing}]{zhang2022improved}
Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, and Eric Xing.
  2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=8lNy3QCaxHX} {Improved logical
  reasoning of language models via differentiable symbolic programming}.
\newblock In \emph{First Workshop on Pre-training: Perspectives, Pitfalls, and
  Paths Forward at ICML 2022}.

\bibitem[{Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura,
  Sridhar, Wang, and Zettlemoyer}]{metaopt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022{\natexlab{b}}.
\newblock {OPT: Open Pre-trained Transformer Language Models}.
\newblock \emph{ArXiv}, abs/2205.01068.

\bibitem[{Zhong et~al.(2022)Zhong, Wang, Tang, Xu, Guo, Chen, Wang, Yin, Zhou,
  and Duan}]{arlsat}
Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai
  Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022.
\newblock Analytical reasoning of text.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  NAACL (NAACL Findings)}.

\bibitem[{Zhou et~al.(2022)Zhou, Scharli, Hou, Wei, Scales, Wang, Schuurmans,
  Bousquet, Le, and Chi}]{LeasttoMostPE}
Denny Zhou, Nathanael Scharli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang,
  Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed~Chi. 2022.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock \emph{ArXiv}, abs/2205.10625.

\end{thebibliography}
