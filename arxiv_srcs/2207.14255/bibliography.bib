@article{hurdles,
  title={Hurdles to Progress in Long-form Question Answering},
  author={Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2103.06332.pdf}{\color{cyan}arXiv:2103.06332}},
  year={2021}
}

@article{eli5,
  title={{ELI5}: Long form question answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1907.09190.pdf}{\color{cyan}arXiv:1907.09190}},
  year={2019}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2005.14165.pdf}{\color{cyan}arXiv:2005.14165}},
  year={2020}
}

@article{hernandez2021,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@article{musenet,
title= {{MuseNet}},
author={Payne, Christine},
journal={{OpenAI}},
url = {openai.com/blog/musenet/},
year={2019}
}

@article{glam,
  title={GLaM: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021}
}

@article{megatron,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{scaling_laws,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{goog_prog_syn,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{instruct,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{summarization,
  title={Learning to summarize from human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2009.01325.pdf}{\color{cyan}arXiv:2009.01325}},
  year={2020}
}

@article{gopher,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{relattn,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{chinchi,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{jur,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  year={2021}
}

@article{palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{truthfulqa,
  title={Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2109.07958.pdf}{\color{cyan}arXiv:2109.07958}},
  year={2021}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2005.11401.pdf}{\color{cyan}arXiv:2005.11401}},
  year={2020}
}

@article{mass,
  title={Mass: Masked sequence to sequence pre-training for language generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1905.02450},
  year={2019}
}

@article{realm,
  title={{REALM}: Retrieval-augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2002.08909.pdf}{\color{cyan}arXiv:2002.08909}},
  year={2020}
}

@article{triviaqa,
  title={Trivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1705.03551.pdf}{\color{cyan}arXiv:1705.03551}},
  year={2017}
}

@article{arc,
  title={Think you have Solved Direct-Answer Question Answering? {T}ry {ARC-DA}, the Direct-Answer {AI2} Reasoning Challenge},
  author={Bhakthavatsalam, Sumithra and Khashabi, Daniel and Khot, Tushar and Mishra, Bhavana Dalvi and Richardson, Kyle and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind and Clark, Peter},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2102.03315.pdf}{\color{cyan}arXiv:2102.03315}},
  year={2021}
}

@article{qaoverlap,
  title={Question and answer test-train overlap in open-domain question answering datasets},
  author={Lewis, Patrick and Stenetorp, Pontus and Riedel, Sebastian},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2008.02637.pdf}{\color{cyan}arXiv:2008.02637}},
  year={2020}
}

@article{unitedqa,
  title={United{QA}: A Hybrid Approach for Open Domain Question Answering},
  author={Cheng, Hao and Shen, Yelong and Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2101.00178.pdf}{\color{cyan}arXiv:2101.00178}},
  year={2021}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1707.06347.pdf}{\color{cyan}arXiv:1707.06347}},
  year={2017}
}

@article{truthfulai,
  title={Truthful {AI}: Developing and governing {AI} that does not lie},
  author={Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2110.06674.pdf}{\color{cyan}arXiv:2110.06674}},
  year={2021}
}

@article{hallucinations,
  title={On faithfulness and factuality in abstractive summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2005.00661.pdf}{\color{cyan}arXiv:2005.00661}},
  year={2020}
}

@article{raghallucinations,
  title={Retrieval Augmentation Reduces Hallucination in Conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2104.07567.pdf}{\color{cyan}arXiv:2104.07567}},
  year={2021}
}

@article{amplification,
  title={Supervising strong learners by amplifying weak experts},
  author={Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1810.08575.pdf}{\color{cyan}arXiv:1810.08575}},
  year={2018}
}

@article{debate,
  title={{AI} safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1805.00899.pdf}{\color{cyan}arXiv:1805.00899}},
  year={2018}
}

@article{rrm,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1811.07871.pdf}{\color{cyan}arXiv:1811.07871}},
  year={2018}
}

@book{superintelligence,
  title={Superintelligence: Paths, Dangers, Strategies},
  author={Bostrom, Nick},
  year={2014},
  publisher={Oxford University Press}
}

@article{rethinkingsearch,
  title={Rethinking Search: Making Experts out of Dilettantes},
  author={Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2105.02274.pdf}{\color{cyan}arXiv:2105.02274}},
  year={2021}
}

@article{perez2019finding,
  title={Finding generalizable evidence by learning to convince q\&a models},
  author={Perez, Ethan and Karamcheti, Siddharth and Fergus, Rob and Weston, Jason and Kiela, Douwe and Cho, Kyunghyun},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1909.05863.pdf}{\color{cyan}arXiv:1909.05863}},
  year={2019}
}

@article{automationbias,
  title={Automation bias: a systematic review of frequency, effect mediators, and mitigators},
  author={Goddard, Kate and Roudsari, Abdul and Wyatt, Jeremy C},
  journal={Journal of the American Medical Informatics Association},
  volume={19},
  number={1},
  pages={121--127},
  year={2012},
  publisher={BMJ Group BMA House, Tavistock Square, London, WC1H 9JR}
}

@article{polyak,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@book{crystalsociety,
  title={Crystal Society},
  author={Harms, M.},
  isbn={9781530773718},
  series={Crystal Trilogy},
  year={2016},
  publisher={CreateSpace Independent Publishing Platform}
}

@article{ferrucci2010building,
  title={Building Watson: An overview of the DeepQA project},
  author={Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John and others},
  journal={AI magazine},
  volume={31},
  number={3},
  pages={59--79},
  year={2010}
}

@article{adolphs2021boosting,
  title={Boosting search engines with interactive agents},
  author={Adolphs, Leonard and Boerschinger, Benjamin and Buck, Christian and Huebscher, Michelle Chen and Ciaramita, Massimiliano and Espeholt, Lasse and Hofmann, Thomas and Kilcher, Yannic},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2109.00527.pdf}{\color{cyan}arXiv:2109.00527}},
  year={2021}
}

@article{yuan2019interactive,
  title={Interactive machine comprehension with information seeking agents},
  author={Yuan, Xingdi and Fu, Jie and Cote, Marc-Alexandre and Tay, Yi and Pal, Christopher and Trischler, Adam},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1908.10449.pdf}{\color{cyan}arXiv:1908.10449}},
  year={2019}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint \href{https://arxiv.org/pdf/2004.04906.pdf}{\color{cyan}arXiv:2004.04906}},
  year={2020}
}

@article{narasimhan2016improving,
  title={Improving information extraction by acquiring external evidence with reinforcement learning},
  author={Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1603.07954.pdf}{\color{cyan}arXiv:1603.07954}},
  year={2016}
}

@inproceedings{shi2017world,
  title={World of bits: An open-domain platform for web-based agents},
  author={Shi, Tianlin and Karpathy, Andrej and Fan, Linxi and Hernandez, Jonathan and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={3135--3144},
  year={2017},
  organization={PMLR}
}

@article{gur2018learning,
  title={Learning to navigate the web},
  author={Gur, Izzeddin and Rueckert, Ulrich and Faust, Aleksandra and Hakkani-Tur, Dilek},
  journal={arXiv preprint \href{https://arxiv.org/pdf/1812.09195.pdf}{\color{cyan}arXiv:1812.09195}},
  year={2018}
}

@article{framingtheory,
  title={Framing theory},
  author={Chong, Dennis and Druckman, James N},
  journal={Annu. Rev. Polit. Sci.},
  volume={10},
  pages={103--126},
  year={2007},
  publisher={Annual Reviews}
}

@article{edit_insert,
title= {{New GPT-3 Capabilities: Edit and Insert}},
author={OpenAI and Bavarian, Mohammad and Jiang, Angela and Jun, Heewoo and Pondé, Henrique},
journal={{OpenAI} blog},
url="https://openai.com/blog/gpt-3-edit-insert/",
year={2022}
}

@inproceedings{lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}

@inproceedings{hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
    abstract = "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as {``}A woman sits at a piano,{''} a machine must select the most likely followup: {``}She sets her fingers on the keys.{''} With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95{\%} accuracy), state-of-the-art models struggle ({\textless}48{\%}). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical {`}Goldilocks{'} zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
}

@inproceedings{storycloze,
    title = "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    author = "Mostafazadeh, Nasrin  and
      Chambers, Nathanael  and
      He, Xiaodong  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Vanderwende, Lucy  and
      Kohli, Pushmeet  and
      Allen, James",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1098",
    doi = "10.18653/v1/N16-1098",
    pages = "839--849",
}

@incollection{winograd,
  abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
  added-at = {2019-01-10T12:03:51.000+0100},
  address = {Rome, Italy},
  author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
  biburl = {https://www.bibsonomy.org/bibtex/2f2adaaa66a83d35ce30618142dcfdbd9/lepsky},
  booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
  interhash = {bf16118d357c0acb5b4463889e76beb4},
  intrahash = {f2adaaa66a83d35ce30618142dcfdbd9},
  isbn = {978-1-57735-560-1},
  keywords = {kuenstliche_intelligenz},
  pages = {552--561},
  publisher = {AAAI Press},
  series = {{KR}'12},
  timestamp = {2019-01-10T12:05:20.000+0100},
  title = {The {Winograd} {Schema} {Challenge}},
  url = {https://cs.nyu.edu/faculty/davise/papers/WSKR2012.pdf},
  urldate = {2019-01-06},
  year = 2012
}

@article{winogrande,
author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3474381},
doi = {10.1145/3474381},
abstract = {Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense.To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4%-79.1%) on WINOGRANDE compared to humans (94%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset.Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
journal = {Commun. ACM},
month = {aug},
pages = {99–106},
numpages = {8}
}

@inproceedings{piqa,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@inproceedings{drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1246",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
    abstract = "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4{\%} F1 on our generalized accuracy metric, while expert human performance is 96{\%}. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51{\%} F1.",
}

@inproceedings{quac,
    title = "{Q}u{AC}: Question Answering in Context",
    author = "Choi, Eunsol  and
      He, He  and
      Iyyer, Mohit  and
      Yatskar, Mark  and
      Yih, Wen-tau  and
      Choi, Yejin  and
      Liang, Percy  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1241",
    doi = "10.18653/v1/D18-1241",
    pages = "2174--2184",
    abstract = "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at \url{http://quac.ai}.",
}

@article{codex,
  author    = {Mark Chen and
               Jerry Tworek and
               Heewoo Jun and
               Qiming Yuan and
               Henrique Ponde de Oliveira Pinto and
               Jared Kaplan and
               Harrison Edwards and
               Yuri Burda and
               Nicholas Joseph and
               Greg Brockman and
               Alex Ray and
               Raul Puri and
               Gretchen Krueger and
               Michael Petrov and
               Heidy Khlaaf and
               Girish Sastry and
               Pamela Mishkin and
               Brooke Chan and
               Scott Gray and
               Nick Ryder and
               Mikhail Pavlov and
               Alethea Power and
               Lukasz Kaiser and
               Mohammad Bavarian and
               Clemens Winter and
               Philippe Tillet and
               Felipe Petroski Such and
               Dave Cummings and
               Matthias Plappert and
               Fotios Chantzis and
               Elizabeth Barnes and
               Ariel Herbert{-}Voss and
               William Hebgen Guss and
               Alex Nichol and
               Alex Paino and
               Nikolas Tezak and
               Jie Tang and
               Igor Babuschkin and
               Suchir Balaji and
               Shantanu Jain and
               William Saunders and
               Christopher Hesse and
               Andrew N. Carr and
               Jan Leike and
               Joshua Achiam and
               Vedant Misra and
               Evan Morikawa and
               Alec Radford and
               Matthew Knight and
               Miles Brundage and
               Mira Murati and
               Katie Mayer and
               Peter Welinder and
               Bob McGrew and
               Dario Amodei and
               Sam McCandlish and
               Ilya Sutskever and
               Wojciech Zaremba},
  title     = {Evaluating Large Language Models Trained on Code},
  journal   = {CoRR},
  volume    = {abs/2107.03374},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.03374},
  eprinttype = {arXiv},
  eprint    = {2107.03374},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{coqa,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1016",
    doi = "10.1162/tacl_a_00266",
    pages = "249--266",
    abstract = "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4{\%}, which is 23.4 points behind human performance (88.8{\%}), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa.",
}

@misc{incoder,
  doi = {10.48550/ARXIV.2204.05999},
  
  url = {https://arxiv.org/abs/2204.05999},
  
  author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  
  keywords = {Software Engineering (cs.SE), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {InCoder: A Generative Model for Code Infilling and Synthesis},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{transfer,
  author    = {Danny Hernandez and
               Jared Kaplan and
               Tom Henighan and
               Sam McCandlish},
  title     = {Scaling Laws for Transfer},
  journal   = {CoRR},
  volume    = {abs/2102.01293},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.01293},
  eprinttype = {arXiv},
  eprint    = {2102.01293},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-01293.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{subword,
  author    = {Rico Sennrich and
               Barry Haddow and
               Alexandra Birch},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  journal   = {CoRR},
  volume    = {abs/1508.07909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.07909},
  eprinttype = {arXiv},
  eprint    = {1508.07909},
  timestamp = {Mon, 13 Aug 2018 16:47:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SennrichHB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bpe-dropout,
  author    = {Ivan Provilkov and
               Dmitrii Emelianenko and
               Elena Voita},
  title     = {BPE-Dropout: Simple and Effective Subword Regularization},
  journal   = {CoRR},
  volume    = {abs/1910.13267},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.13267},
  eprinttype = {arXiv},
  eprint    = {1910.13267},
  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13267.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{ctrl,
  author    = {Nitish Shirish Keskar and
               Bryan McCann and
               Lav R. Varshney and
               Caiming Xiong and
               Richard Socher},
  title     = {{CTRL:} {A} Conditional Transformer Language Model for Controllable
               Generation},
  journal   = {CoRR},
  volume    = {abs/1909.05858},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.05858},
  eprinttype = {arXiv},
  eprint    = {1909.05858},
  timestamp = {Wed, 18 Sep 2019 10:38:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-05858.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{distaug,
  title = 	 {Distribution Augmentation for Generative Modeling},
  author =       {Jun, Heewoo and Child, Rewon and Chen, Mark and Schulman, John and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5006--5019},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jun20a/jun20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/jun20a.html},
  abstract = 	 {We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Wed, 16 Mar 2022 23:55:36 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{spanbert,
    title = "{S}pan{BERT}: Improving Pre-training by Representing and Predicting Spans",
    author = "Joshi, Mandar  and
      Chen, Danqi  and
      Liu, Yinhan  and
      Weld, Daniel S.  and
      Zettlemoyer, Luke  and
      Levy, Omer",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.5",
    doi = "10.1162/tacl_a_00300",
    pages = "64--77",
    abstract = "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6{\%} and 88.7{\%} F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\%} F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1",
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{dobf,
  author    = {Marie{-}Anne Lachaux and
               Baptiste Rozi{\`{e}}re and
               Marc Szafraniec and
               Guillaume Lample},
  editor    = {Marc'Aurelio Ranzato and
               Alina Beygelzimer and
               Yann N. Dauphin and
               Percy Liang and
               Jennifer Wortman Vaughan},
  title     = {{DOBF:} {A} Deobfuscation Pre-Training Objective for Programming Languages},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {14967--14979},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html},
  timestamp = {Tue, 03 May 2022 16:20:48 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/LachauxRSL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{htlm,
  author    = {Armen Aghajanyan and
               Dmytro Okhonko and
               Mike Lewis and
               Mandar Joshi and
               Hu Xu and
               Gargi Ghosh and
               Luke Zettlemoyer},
  title     = {{HTLM:} Hyper-Text Pre-Training and Prompting of Language Models},
  journal   = {CoRR},
  volume    = {abs/2107.06955},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.06955},
  eprinttype = {arXiv},
  eprint    = {2107.06955},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-06955.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inbook{xlnet,
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {517},
numpages = {11}
}


@InProceedings{insertion-transformer,
  title = 	 {Insertion Transformer: Flexible Sequence Generation via Insertion Operations},
  author =       {Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5976--5985},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/stern19a/stern19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/stern19a.html},
  abstract = 	 {We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a fixed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow specific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.}
}

@article{kermit,
  author    = {William Chan and
               Nikita Kitaev and
               Kelvin Guu and
               Mitchell Stern and
               Jakob Uszkoreit},
  title     = {{KERMIT:} Generative Insertion-Based Modeling for Sequences},
  journal   = {CoRR},
  volume    = {abs/1906.01604},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.01604},
  eprinttype = {arXiv},
  eprint    = {1906.01604},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-01604.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{indigo,
    title = "Insertion-based Decoding with Automatically Inferred Generation Order",
    author = "Gu, Jiatao  and
      Liu, Qi  and
      Cho, Kyunghyun",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1042",
    doi = "10.1162/tacl_a_00292",
    pages = "661--676",
    abstract = "Conventional neural autoregressive decoding commonly assumes a fixed left-to-right generation order, which may be sub-optimal. In this work, we propose a novel decoding algorithm{---} InDIGO{---}which supports flexible sequence generation in arbitrary orders through insertion operations. We extend Transformer, a state-of-the-art sequence generation model, to efficiently implement the proposed approach, enabling it to be trained with either a pre-defined generation order or adaptive orders obtained from beam-search. Experiments on four real-world tasks, including word order recovery, machine translation, image caption, and code generation, demonstrate that our algorithm can generate sequences following arbitrary orders, while achieving competitive or even better performance compared with the conventional left-to-right generation. The generated sequences show that InDIGO adopts adaptive generation orders based on input information.",
}

@inproceedings{blanklm,
    title = "Blank Language Models",
    author = "Shen, Tianxiao  and
      Quach, Victor  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.420",
    doi = "10.18653/v1/2020.emnlp-main.420",
    pages = "5186--5198",
    abstract = "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.",
}

@article{donahue,
  author    = {Chris Donahue and
               Mina Lee and
               Percy Liang},
  title     = {Enabling Language Models to Fill in the Blanks},
  journal   = {CoRR},
  volume    = {abs/2005.05339},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.05339},
  eprinttype = {arXiv},
  eprint    = {2005.05339},
  timestamp = {Thu, 14 May 2020 16:56:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-05339.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhu-infilling,
  author    = {Wanrong Zhu and
               Zhiting Hu and
               Eric P. Xing},
  title     = {Text Infilling},
  journal   = {CoRR},
  volume    = {abs/1901.00158},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.00158},
  eprinttype = {arXiv},
  eprint    = {1901.00158},
  timestamp = {Mon, 22 Jul 2019 16:49:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-00158.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glm,
    title = "{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling",
    author = "Du, Zhengxiao  and
      Qian, Yujie  and
      Liu, Xiao  and
      Ding, Ming  and
      Qiu, Jiezhong  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.26",
    doi = "10.18653/v1/2022.acl-long.26",
    pages = "320--335",
    abstract = "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\mbox{$\times$}} parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
}

@article{cm3,
  author    = {Armen Aghajanyan and
               Bernie Huang and
               Candace Ross and
               Vladimir Karpukhin and
               Hu Xu and
               Naman Goyal and
               Dmytro Okhonko and
               Mandar Joshi and
               Gargi Ghosh and
               Mike Lewis and
               Luke Zettlemoyer},
  title     = {{CM3:} {A} Causal Masked Multimodal Model of the Internet},
  journal   = {CoRR},
  volume    = {abs/2201.07520},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.07520},
  eprinttype = {arXiv},
  eprint    = {2201.07520},
  timestamp = {Fri, 21 Jan 2022 13:57:15 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-07520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{maskgan,
  doi = {10.48550/ARXIV.1801.07736},
  url = {https://arxiv.org/abs/1801.07736},
  author = {Fedus, William and Goodfellow, Ian and Dai, Andrew M.},
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MaskGAN: Better Text Generation via Filling in the\_\_\_\_\_\_},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tigs,
    title = "{TIGS}: An Inference Algorithm for Text Infilling with Gradient Search",
    author = "Liu, Dayiheng  and
      Fu, Jie  and
      Liu, Pengfei  and
      Lv, Jiancheng",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1406",
    doi = "10.18653/v1/P19-1406",
    pages = "4146--4156",
    abstract = "Text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. Given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. In this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods.",
}



@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@inproceedings{xl,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@misc{bidirlm,
  doi = {10.48550/ARXIV.2205.11726},
  
  url = {https://arxiv.org/abs/2205.11726},
  
  author = {Artetxe, Mikel and Du, Jingfei and Goyal, Naman and Zettlemoyer, Luke and Stoyanov, Ves},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Role of Bidirectionality in Language Model Pre-Training},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{huggingface,
      title={What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?}, 
      author={Thomas Wang and Adam Roberts and Daniel Hesslow and Teven Le Scao and Hyung Won Chung and Iz Beltagy and Julien Launay and Colin Raffel},
      year={2022},
      eprint={2204.05832},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{reasonbert,
  author    = {Xiang Deng and
               Yu Su and
               Alyssa Lees and
               You Wu and
               Cong Yu and
               Huan Sun},
  title     = {ReasonBERT: Pre-trained to Reason with Distant Supervision},
  journal   = {CoRR},
  volume    = {abs/2109.04912},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.04912},
  eprinttype = {arXiv},
  eprint    = {2109.04912},
  timestamp = {Thu, 16 Dec 2021 08:39:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-04912.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{rlhf,
  author    = {Nisan Stiennon and
               Long Ouyang and
               Jeff Wu and
               Daniel M. Ziegler and
               Ryan Lowe and
               Chelsea Voss and
               Alec Radford and
               Dario Amodei and
               Paul F. Christiano},
  title     = {Learning to summarize from human feedback},
  journal   = {CoRR},
  volume    = {abs/2009.01325},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.01325},
  eprinttype = {arXiv},
  eprint    = {2009.01325},
  timestamp = {Thu, 01 Apr 2021 19:06:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-01325.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{if,
  doi = {10.48550/ARXIV.2203.02155},
  
  url = {https://arxiv.org/abs/2203.02155},
  
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training language models to follow instructions with human feedback},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{nucleus,
  added-at = {2021-01-26T15:34:35.000+0100},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  biburl = {https://www.bibsonomy.org/bibtex/229fdab5aeb4deb740ceb07a72a6e9058/albinzehe},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=rygGQyrFvH},
  interhash = {1c12da68a933353ff24dd2f8fc669961},
  intrahash = {29fdab5aeb4deb740ceb07a72a6e9058},
  keywords = {degeneration dfg-antrag-steckbriefe kallimachos languagemodel neuralnets text},
  publisher = {OpenReview.net},
  timestamp = {2021-01-26T15:34:35.000+0100},
  title = {The Curious Case of Neural Text Degeneration.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2020.html#HoltzmanBDFC20},
  year = 2020
}



@misc{unifying,
  doi = {10.48550/ARXIV.2205.05131},
  
  url = {https://arxiv.org/abs/2205.05131},
  
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unifying Language Learning Paradigms},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
